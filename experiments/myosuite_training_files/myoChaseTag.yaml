DEP:
  bias_rate: 0.002
  buffer_size: 200
  intervention_length: 5
  intervention_proba: 0.0004
  kappa: 1169.7
  normalization: independent
  q_norm_selector: l2
  regularization: 32
  s4avg: 2
  sensor_delay: 1
  tau: 40
  test_episode_every: 3
  time_dist: 5
  with_learning: true
env_args: {}
mpo_args:
  hidden_size: 1024
  lr_actor: 3.53e-05
  lr_critic: 6.081e-05
  lr_dual: 0.00213
tonic:
  after_training: ''
  agent: deprl.custom_agents.dep_factory(3, deprl.custom_mpo_torch.TunedMPO())(replay=deprl.replays.buffers.Buffer(return_steps=3,
    batch_size=256, steps_between_batches=1000, batch_iterations=30, steps_before_batches=2e5))
  before_training: ''
  checkpoint: last
  environment: deprl.environments.Gym('myoChallengeChaseTagP1-v0', scaled_actions=False,
    reset_type='random')
  environment_name: deprl_baseline_chasetag
  full_save: 1
  header: import deprl, gym, myosuite
  name: myoChasetag
  parallel: 20
  path: ./output
  resume: 1
  seed: 0
  sequential: 10
  test_environment: null
  trainer: deprl.custom_trainer.Trainer(steps=int(1e8), epoch_steps=int(2e5), save_steps=int(1e6))
working_dir: ./baselines_DEPRL
